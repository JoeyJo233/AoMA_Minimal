{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt  # 新增：用于可视化\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class PatrolEnvTimeUnified:\n",
    "    \"\"\"\n",
    "    改进版的安保巡逻环境：\n",
    "      - 有 3 个主地点: 0->L0, 1->L1, 2->Home\n",
    "      - 当执行 \"Go L0\" 或 \"Go L1\" 或 \"Go Home\" 时，会用子时间步(sub-steps)来模拟实际行走时间\n",
    "      - 在每个子时间步内，警报 AoMA、警报产生、电量消耗等都会被更新\n",
    "      - 这样就实现了“移动过程中 AoMA 也会持续增长、能耗也随时间分步扣除”的效果\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 # ---- 警报产生概率 ----\n",
    "                 p0=0.05, p1=0.05,\n",
    "                 # ---- 时间 / 能量设置 ----\n",
    "                 # time_xxx_yyy 表示从 xxx -> yyy 需要多少子时间步\n",
    "                 # energy_xxx_yyy 表示从 xxx -> yyy 的总能耗（会在子步中分摊）\n",
    "                 time_home_l0=3, energy_home_l0=3,\n",
    "                 time_home_l1=4, energy_home_l1=4,\n",
    "                 time_l0_l1=2,   energy_l0_l1=2,\n",
    "                 # ---- 其它能耗 / 充电设置 ----\n",
    "                 idle_cost=1,           # 在 L0/L1 等待时每子步消耗\n",
    "                 charge_rate=5,         # 在 Home 等待每子步充多少电\n",
    "                 battery_capacity=20,   # 电池容量\n",
    "                 b_safe=5,              # 低电量阈值\n",
    "                 # ---- AoMA / 奖励参数 ----\n",
    "                 beta=1.5,   # 未处理警报的按步负奖励系数\n",
    "                 gamma=20,  # 低电量惩罚\n",
    "                 M=200,       # 抛锚大惩罚\n",
    "                 rho=20,    # 清除警报时的小正奖励\n",
    "                 max_aoma=40, # AoMA 上限 (避免数字过大)\n",
    "                 max_steps=500, # 每个 Episode 的最大(大)步数(Agent层面的步数)\n",
    "                 seed=0\n",
    "                 ):\n",
    "        print(\"rho:\",rho)\n",
    "        print(\"抛锚惩罚:\",M)\n",
    "\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        self.rng = np.random.RandomState(seed)\n",
    "\n",
    "        # 警报概率\n",
    "        self.p0 = p0\n",
    "        self.p1 = p1\n",
    "\n",
    "        # 移动时间与能耗矩阵 (可以根据需要扩充)\n",
    "        # 这里简单处理：从 i->j 的时间 & 能耗与 j->i 相同\n",
    "        self.time_home_l0 = time_home_l0\n",
    "        self.energy_home_l0 = energy_home_l0\n",
    "        self.time_home_l1 = time_home_l1\n",
    "        self.energy_home_l1 = energy_home_l1\n",
    "        self.time_l0_l1   = time_l0_l1\n",
    "        self.energy_l0_l1 = energy_l0_l1\n",
    "\n",
    "        self.idle_cost = idle_cost\n",
    "        self.charge_rate = charge_rate\n",
    "        self.battery_capacity = battery_capacity\n",
    "        self.b_safe = b_safe\n",
    "\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "        self.M = M\n",
    "        self.rho = rho\n",
    "        self.max_aoma = max_aoma\n",
    "        self.max_steps = max_steps\n",
    "        self.current_step = 0  # 大步计数(Agent每执行一次env.step就+1)\n",
    "\n",
    "        # 状态： AoMA_0, AoMA_1, loc ∈ {0,1,2}, battery\n",
    "        self.state_dim = 4\n",
    "        # 动作：0->Go L0, 1->Go L1, 2->Go Home, 3->Wait\n",
    "        self.action_space = 4\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"重置环境到初始状态，返回初始观测\"\"\"\n",
    "        self.AoMA_0 = 0\n",
    "        self.AoMA_1 = 0\n",
    "        self.loc = 2  # 初始在Home\n",
    "        self.battery = self.battery_capacity\n",
    "        self.current_step = 0\n",
    "        return self._get_state()\n",
    "\n",
    "    def _get_state(self):\n",
    "        return np.array([self.AoMA_0, self.AoMA_1, self.loc, self.battery], dtype=float)\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        执行动作，并做多次“子时间步”循环:\n",
    "          - 例如, \"Go L1\" 如果 time_home_l1 = 4, 就子循环4次\n",
    "          - 每个子时间步中, 更新警报(产生/累加AoMA), 更新电量, 检查抛锚\n",
    "          - 全部子时间步结束后, 才真正到达目标地点\n",
    "        最终返回 (next_state, reward, done, info)\n",
    "        \"\"\"\n",
    "        self.current_step += 1  # 这里是“大步”计数(Agent每决策一次)\n",
    "        done = False\n",
    "        total_sub_reward = 0.0  # 累加子步的即时奖励\n",
    "\n",
    "        # 1. 根据动作, 确定移动需要的子步数 sub_steps 以及每子步能耗\n",
    "        if action == 0:  # Go L0\n",
    "            # 分情况：从哪来？\n",
    "            if self.loc == 2:  # Home->L0\n",
    "                sub_steps = self.time_home_l0\n",
    "                total_energy = self.energy_home_l0\n",
    "            elif self.loc == 1: # L1->L0\n",
    "                sub_steps = self.time_l0_l1\n",
    "                total_energy = self.energy_l0_l1\n",
    "            else:  # 本来就在 L0\n",
    "                sub_steps = 0\n",
    "                total_energy = 0\n",
    "\n",
    "            # 把 total_energy 分到每个子步 (假设能整除, 否则可做浮点除)\n",
    "            energy_per_step = 0\n",
    "            if sub_steps > 0:\n",
    "                energy_per_step = float(total_energy) / float(sub_steps)\n",
    "\n",
    "            # 进入子步循环\n",
    "            for _ in range(sub_steps):\n",
    "                # 每个子步都更新警报, 电量, 奖励\n",
    "                r_sub, done = self._do_sub_step(moving=True, cost=energy_per_step)\n",
    "                total_sub_reward += r_sub\n",
    "                if done:\n",
    "                    break\n",
    "            # 若没抛锚, 最终到达 L0\n",
    "            if not done:\n",
    "                self.loc = 0\n",
    "\n",
    "        elif action == 1:  # Go L1\n",
    "            if self.loc == 2:  # Home->L1\n",
    "                sub_steps = self.time_home_l1\n",
    "                total_energy = self.energy_home_l1\n",
    "            elif self.loc == 0: # L0->L1\n",
    "                sub_steps = self.time_l0_l1\n",
    "                total_energy = self.energy_l0_l1\n",
    "            else:  # 本来就在L1\n",
    "                sub_steps = 0\n",
    "                total_energy = 0\n",
    "\n",
    "            energy_per_step = 0\n",
    "            if sub_steps > 0:\n",
    "                energy_per_step = float(total_energy)/float(sub_steps)\n",
    "\n",
    "            for _ in range(sub_steps):\n",
    "                r_sub, done = self._do_sub_step(moving=True, cost=energy_per_step)\n",
    "                total_sub_reward += r_sub\n",
    "                if done:\n",
    "                    break\n",
    "            if not done:\n",
    "                self.loc = 1\n",
    "\n",
    "        elif action == 2:  # Go Home\n",
    "            if self.loc == 0:\n",
    "                sub_steps = self.time_home_l0\n",
    "                total_energy = self.energy_home_l0\n",
    "            elif self.loc == 1:\n",
    "                sub_steps = self.time_home_l1\n",
    "                total_energy = self.energy_home_l1\n",
    "            else:\n",
    "                sub_steps = 0\n",
    "                total_energy = 0\n",
    "\n",
    "            energy_per_step = 0\n",
    "            if sub_steps > 0:\n",
    "                energy_per_step = float(total_energy)/float(sub_steps)\n",
    "\n",
    "            for _ in range(sub_steps):\n",
    "                r_sub, done = self._do_sub_step(moving=True, cost=energy_per_step)\n",
    "                total_sub_reward += r_sub\n",
    "                if done:\n",
    "                    break\n",
    "            if not done:\n",
    "                self.loc = 2\n",
    "\n",
    "        elif action == 3:  # Wait\n",
    "            # 等待 1 个子步\n",
    "            # 在 Home 等待 => 充电; 在 L0/L1 等待 => idle_cost\n",
    "            r_sub, done = self._do_sub_step(moving=False, cost=self.idle_cost)\n",
    "            total_sub_reward += r_sub\n",
    "        else:\n",
    "            raise ValueError(\"Invalid action!\")\n",
    "\n",
    "        # 2. 如果在子步中已经 done，就直接返回\n",
    "        if done:\n",
    "            return self._get_state(), total_sub_reward, True, {}\n",
    "\n",
    "        # 3. 如果移动成功抵达了有警报的地点 => 清除警报 => +rho (一次性的)\n",
    "        reward_clear = 0.0\n",
    "        if self.loc == 0 and self.AoMA_0 > 0:\n",
    "            self.AoMA_0 = 0\n",
    "            reward_clear += self.rho\n",
    "        if self.loc == 1 and self.AoMA_1 > 0:\n",
    "            self.AoMA_1 = 0\n",
    "            reward_clear += self.rho\n",
    "\n",
    "        # 4. 把清除警报的奖励加上\n",
    "        total_sub_reward += reward_clear\n",
    "\n",
    "        # 5. 最后再判断是否超过最大大步数\n",
    "        if self.current_step >= self.max_steps:\n",
    "            done = True\n",
    "\n",
    "        return self._get_state(), total_sub_reward, done, {}\n",
    "\n",
    "    def _do_sub_step(self, moving, cost):\n",
    "        \"\"\"\n",
    "        执行一个“子时间步”：\n",
    "          - 随机产生新警报\n",
    "          - 未处理警报 AoMA += 1\n",
    "          - 若在移动, 电量扣 cost(或 cost 的整数分配); 若在Home等待, 就充电; 若在 L0/L1 等待, 扣 idle_cost\n",
    "          - 计算当下的即时奖励(未处理警报惩罚 + 低电量惩罚)\n",
    "          - 如果电量不够 => 抛锚 done\n",
    "        返回 (reward_sub, done)\n",
    "        \"\"\"\n",
    "        done = False\n",
    "        reward_sub = 0.0\n",
    "\n",
    "        # a) 产生新警报(仅当 AoMA==0 的地点才可能出现新警报)\n",
    "        if self.AoMA_0 == 0:\n",
    "            if self.rng.rand() < self.p0:\n",
    "                self.AoMA_0 = 1\n",
    "        else:\n",
    "            self.AoMA_0 = min(self.AoMA_0 + 1, self.max_aoma)\n",
    "\n",
    "        if self.AoMA_1 == 0:\n",
    "            if self.rng.rand() < self.p1:\n",
    "                self.AoMA_1 = 1\n",
    "        else:\n",
    "            self.AoMA_1 = min(self.AoMA_1 + 1, self.max_aoma)\n",
    "\n",
    "        # b) 根据地点 & 是否移动, 扣/加电量\n",
    "        if moving:\n",
    "            # 如果 cost 不是整数, 这里做个向上取整或者直接用浮点\n",
    "            # 为简单起见, 这里要求 cost <= battery, 然后一次扣 cost (也可 cost=1, 循环多次)\n",
    "            if self.battery < cost:\n",
    "                # 抛锚\n",
    "                return -self.M, True\n",
    "            else:\n",
    "                self.battery -= cost\n",
    "        else:\n",
    "            # 不移动 => 可能在 L0/L1 等待 or Home 等待\n",
    "            if self.loc in [0,1]:\n",
    "                # idle_cost = cost\n",
    "                if self.battery < cost:\n",
    "                    return -self.M, True\n",
    "                self.battery -= cost\n",
    "            else:\n",
    "                # 在 Home => 充电\n",
    "                self.battery = min(self.battery + self.charge_rate, self.battery_capacity)\n",
    "\n",
    "        # c) 计算该子步的即时奖励\n",
    "        n_alarms = 0\n",
    "        if self.AoMA_0 > 0:\n",
    "            n_alarms += 1\n",
    "        if self.AoMA_1 > 0:\n",
    "            n_alarms += 1\n",
    "\n",
    "        r_alarm = -self.beta * n_alarms\n",
    "        r_battery = -self.gamma if self.battery < self.b_safe else 0.0\n",
    "\n",
    "        reward_sub = r_alarm + r_battery\n",
    "\n",
    "        return reward_sub, False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------\n",
    "# 2. Q 网络 (DQN)\n",
    "# -------------------------------------------------\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=64):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, action_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 3. DQN Agent\n",
    "# -------------------------------------------------\n",
    "class DQNAgent:\n",
    "    def __init__(self,\n",
    "                 state_dim,\n",
    "                 action_dim,\n",
    "                 hidden_dim=64,\n",
    "                 lr=1e-3,\n",
    "                 gamma=0.99,\n",
    "                 epsilon_start=1.0,\n",
    "                 epsilon_end=0.01,\n",
    "                 epsilon_decay=5000,\n",
    "                 buffer_size=50000,\n",
    "                 batch_size=64,\n",
    "                 seed=0):\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "\n",
    "        self.action_dim = action_dim\n",
    "        self.gamma = gamma\n",
    "        self.epsilon_start = epsilon_start\n",
    "        self.epsilon_end   = epsilon_end\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.step_count = 0\n",
    "\n",
    "        self.q_net = QNetwork(state_dim, action_dim, hidden_dim)\n",
    "        self.target_net = QNetwork(state_dim, action_dim, hidden_dim)\n",
    "        self.target_net.load_state_dict(self.q_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "\n",
    "        self.optimizer = optim.Adam(self.q_net.parameters(), lr=lr)\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "        # 经验回放\n",
    "        self.replay_buffer = deque(maxlen=buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def select_action(self, state):\n",
    "        # epsilon-greedy\n",
    "        epsilon = self.epsilon_end + (self.epsilon_start - self.epsilon_end) * \\\n",
    "                  np.exp(-1.0 * self.step_count / self.epsilon_decay)\n",
    "        self.step_count += 1\n",
    "\n",
    "        if random.random() < epsilon:\n",
    "            return random.randint(0, self.action_dim - 1)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                s = torch.FloatTensor(state).unsqueeze(0)\n",
    "                q_values = self.q_net(s)\n",
    "                return q_values.argmax(dim=1).item()\n",
    "\n",
    "    def store_transition(self, s, a, r, s_next, done):\n",
    "        self.replay_buffer.append((s, a, r, s_next, done))\n",
    "\n",
    "    def update(self):\n",
    "        \"\"\" 从回放池采样, 用 MSELoss 更新 Q 网络 \"\"\"\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return  # 数据不够\n",
    "\n",
    "        batch = random.sample(self.replay_buffer, self.batch_size)\n",
    "        s, a, r, s_next, done = zip(*batch)\n",
    "\n",
    "        s      = torch.FloatTensor(s)\n",
    "        a      = torch.LongTensor(a).unsqueeze(1)\n",
    "        r      = torch.FloatTensor(r).unsqueeze(1)\n",
    "        s_next = torch.FloatTensor(s_next)\n",
    "        done   = torch.FloatTensor(done).unsqueeze(1)\n",
    "\n",
    "        # 当前 Q\n",
    "        q_pred = self.q_net(s).gather(1, a)\n",
    "\n",
    "        # 目标 Q (用 target_net)\n",
    "        with torch.no_grad():\n",
    "            q_next = self.target_net(s_next).max(dim=1, keepdim=True)[0]\n",
    "            q_target = r + self.gamma * q_next * (1 - done)\n",
    "\n",
    "        loss = self.criterion(q_pred, q_target)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def soft_update(self, tau=0.01):\n",
    "        \"\"\" 软更新目标网络参数: target <- tau*Q + (1-tau)*target \"\"\"\n",
    "        for param, param_targ in zip(self.q_net.parameters(), self.target_net.parameters()):\n",
    "            param_targ.data.copy_(tau * param.data + (1 - tau) * param_targ.data)\n",
    "#--------用于test--------\n",
    "    def select_action_inference(self, state):\n",
    "        \"\"\"\n",
    "        在测试(推理)阶段用纯贪心选择动作:\n",
    "        不进行 epsilon 探索。\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            s = torch.FloatTensor(state).unsqueeze(0)\n",
    "            q_values = self.q_net(s)  # [1, action_dim]\n",
    "            action = q_values.argmax(dim=1).item()  # int\n",
    "        return action\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------\n",
    "# 4. 辅助函数: 绘制训练曲线\n",
    "# -------------------------------------------------\n",
    "def plot_learning_curve(reward_history, window=50):\n",
    "    \"\"\"\n",
    "    绘制每个Episode的累计奖励, 以及滑动平均曲线\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.plot(reward_history, label='Episode Reward', alpha=0.4)\n",
    "    \n",
    "    # 计算滑动平均\n",
    "    if len(reward_history) >= window:\n",
    "        moving_avg = []\n",
    "        cum_sum = 0\n",
    "        for i, r in enumerate(reward_history):\n",
    "            cum_sum += r\n",
    "            if i >= window:\n",
    "                cum_sum -= reward_history[i-window]\n",
    "            avg = cum_sum / min(i+1, window)\n",
    "            moving_avg.append(avg)\n",
    "        plt.plot(moving_avg, label=f'Moving Avg({window})', color='orange')\n",
    "    \n",
    "    plt.title('Training Reward Over Episodes')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Total Reward')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 5. 训练脚本(带可视化)\n",
    "# -------------------------------------------------\n",
    "def train_dqn(num_episodes=1000):\n",
    "    env = PatrolEnvTimeUnified(\n",
    "        # p0=0.05, p1=0.05,\n",
    "        # E_home_l0=3, E_home_l1=3, E_l0_l1=2,\n",
    "        # idle_cost=1, charge_rate=2,\n",
    "        # battery_capacity=20, b_safe=5,\n",
    "        # beta=1.0, gamma=2.0, M=50, rho=0.0,\n",
    "        # max_aoma=50, max_steps=200,\n",
    "        # seed=42\n",
    "    )\n",
    "\n",
    "    agent = DQNAgent(\n",
    "        state_dim=env.state_dim,\n",
    "        action_dim=env.action_space,\n",
    "        hidden_dim=64, lr=1e-3,\n",
    "        gamma=0.99, epsilon_start=1.0,\n",
    "        epsilon_end=0.01, epsilon_decay=5000,\n",
    "        buffer_size=50000, batch_size=64,\n",
    "        seed=42\n",
    "    )\n",
    "    train_aomas = []\n",
    "\n",
    "    max_steps_per_episode = env.max_steps\n",
    "    reward_history = []\n",
    "\n",
    "    for ep in range(num_episodes):\n",
    "        s = env.reset()\n",
    "        episode_reward = 0\n",
    "\n",
    "        for t in range(max_steps_per_episode):\n",
    "            a = agent.select_action(s)\n",
    "            s_next, r, done, info = env.step(a)\n",
    "            agent.store_transition(s, a, r, s_next, done)\n",
    "\n",
    "            s = s_next\n",
    "            episode_reward += r\n",
    "\n",
    "            agent.update()\n",
    "            agent.soft_update(tau=0.01)\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        reward_history.append(episode_reward)\n",
    "        # 每隔一定回数打印一下信息\n",
    "        if (ep+1) % 100 == 0:\n",
    "            avg_reward = np.mean(reward_history[-100:])\n",
    "            print(f\"Episode {ep+1:4d}, AvgReward(100)={avg_reward:.2f}\")\n",
    "\n",
    "    # 绘制训练曲线\n",
    "    plot_learning_curve(reward_history, window=50)\n",
    "\n",
    "    # 保存模型\n",
    "    torch.save({\n",
    "    \"q_net\": agent.q_net.state_dict(),\n",
    "    \"target_net\": agent.target_net.state_dict(),}, \"trained_agent.pth\")\n",
    "\n",
    "    return agent, reward_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rho: 10\n",
      "抛锚惩罚: 200\n",
      "Episode  100, AvgReward(100)=-380.36\n",
      "Episode  200, AvgReward(100)=-554.36\n",
      "Episode  300, AvgReward(100)=-588.31\n",
      "Episode  400, AvgReward(100)=-184.28\n",
      "Episode  500, AvgReward(100)=-116.67\n",
      "Episode  600, AvgReward(100)=-31.75\n",
      "Episode  700, AvgReward(100)=-24.06\n",
      "Episode  800, AvgReward(100)=-2.35\n",
      "Episode  900, AvgReward(100)=23.63\n",
      "Episode 1000, AvgReward(100)=17.34\n",
      "Episode 1100, AvgReward(100)=11.04\n",
      "Episode 1200, AvgReward(100)=43.91\n",
      "Episode 1300, AvgReward(100)=42.28\n",
      "Episode 1400, AvgReward(100)=63.47\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------\n",
    "# 6. 测试训练并观察结果\n",
    "# -------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    trained_agent, reward_hist = train_dqn(num_episodes=1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_dqn_model(env, agent, num_test_episodes=20, render=False):\n",
    "    \"\"\"\n",
    "    使用传入的 agent(含已加载权重) 在 env 上测试 num_test_episodes 个回合,\n",
    "    并统计:\n",
    "      - 平均 Episode 回报\n",
    "      - 平均 Episode AoMA\n",
    "\n",
    "    参数:\n",
    "      env:  测试环境 (与训练环境相同或类似)\n",
    "      agent: 已含有训练好网络权重的 DQNAgent\n",
    "      num_test_episodes: 测试多少个回合\n",
    "      render: 若需要可视化环境, 这里暂时不做GUI, 仅做示例\n",
    "\n",
    "    返回:\n",
    "      test_rewards: 每个Episode的总回报列表\n",
    "      test_aomas:   每个Episode的平均AoMA列表\n",
    "    \"\"\"\n",
    "    test_rewards = []\n",
    "    test_aomas = []\n",
    "\n",
    "    for episode in range(num_test_episodes):\n",
    "        state = env.reset()\n",
    "        ep_reward = 0.0\n",
    "\n",
    "        while True:\n",
    "            # 1) 用纯贪心策略选动作\n",
    "            action = agent.select_action_inference(state)\n",
    "\n",
    "            # 2) 与环境交互\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            ep_reward += reward\n",
    "\n",
    "            # 如果想打印或渲染细节, 可在此处加入\n",
    "            if render:\n",
    "                # print(\"Step:\", state, action, reward, next_state, done)\n",
    "                pass\n",
    "\n",
    "            state = next_state\n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        test_rewards.append(ep_reward)\n",
    "        avg_aoma = env.get_episode_aoma()  # 本回合统计到的平均AoMA\n",
    "        test_aomas.append(avg_aoma)\n",
    "\n",
    "    # 简单汇总一下结果\n",
    "    mean_reward = np.mean(test_rewards)\n",
    "    mean_aoma   = np.mean(test_aomas)\n",
    "\n",
    "    print(f\"[Test] NumEpisodes={num_test_episodes}\")\n",
    "    print(f\"      Average Reward={mean_reward:.2f}, Average AoMA={mean_aoma:.2f}\")\n",
    "\n",
    "    return test_rewards, test_aomas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) 创建与你训练时相同(或兼容)的环境\n",
    "test_env = PatrolEnvTimeUnified(\n",
    "    p0=0.05, p1=0.05,\n",
    "    time_home_l0=3, energy_home_l0=3,\n",
    "    time_home_l1=4, energy_home_l1=4,\n",
    "    time_l0_l1=2, energy_l0_l1=2,\n",
    "    idle_cost=1, charge_rate=2,\n",
    "    battery_capacity=20, b_safe=5,\n",
    "    beta=1.0, gamma=2.0, M=50, rho=0.0,\n",
    "    max_aoma=999, max_steps=200,\n",
    "    seed=999  # 测试时可以换个随机种子\n",
    ")\n",
    "\n",
    "# 2) 创建一个空白 agent (结构需与训练时一致)\n",
    "test_agent = DQNAgent(\n",
    "    state_dim=test_env.state_dim,\n",
    "    action_dim=test_env.action_space,\n",
    "    hidden_dim=64, lr=1e-3,\n",
    "    gamma=0.99, epsilon_start=1.0, # 这里不会用\n",
    "    epsilon_end=0.01, epsilon_decay=5000,\n",
    "    buffer_size=50000, batch_size=64,\n",
    "    seed=999\n",
    ")\n",
    "\n",
    "# 3) 从文件加载训练好的权重(假设你在训练完后保存了 'trained_agent.pth')\n",
    "checkpoint = torch.load(\"trained_agent.pth\", map_location='cpu')\n",
    "test_agent.q_net.load_state_dict(checkpoint[\"q_net\"])\n",
    "test_agent.target_net.load_state_dict(checkpoint[\"target_net\"])\n",
    "# 也可以加载 step_count 等信息, 看你训练时怎么保存\n",
    "\n",
    "# 4) 测试\n",
    "rewards, aomas = test_dqn_model(\n",
    "    env=test_env,\n",
    "    agent=test_agent,\n",
    "    num_test_episodes=30,  # 测试30回合\n",
    "    render=False\n",
    ")\n",
    "# 这会打印测试时的平均 Reward 和 平均 AoMA"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (base)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
